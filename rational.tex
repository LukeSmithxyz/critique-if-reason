\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[backend=biber, style=authoryear-icomp,maxnames=2]{biblatex}
\usepackage{titlesec}
\usepackage{easylist}
\usepackage{hanging}
\usepackage{hyperref}
\usepackage{blindtext}
\usepackage{tipa}
\usepackage{cgloss4e}
\usepackage{epigraph}
\usepackage{gb4e}
\usepackage{qtree}
\usepackage{enumerate}
\usepackage{longtable}
\usepackage{textgreek}
\addbibresource{$HOME/Documents/LaTeX/uni.bib}

\titleformat{\section}
{\bfseries\centering\large}
{ยง\thesection}
{1em}
{}

\titleformat{\subsection}
{\bfseries\centering}
{ยง\thesubsection}
{1em}
{}

\titleformat{\subsubsection}
{\bfseries\centering}
{ยง\thesubsubsection}
{1em}
{}


\title{A ``Critique'' of ``Reason''}
\author{Luke Smith}

\begin{document}

\maketitle

\begin{quote}
``The aberration of philosophy is that, instead of seeing logic and the categories of reason as means toward the adjustment of the world for utilitarian ends (basically involving an expedient falsification), one believed one possessed in them the criterion of truth and reality.
	[\ldots]
Instead of employing them as a tool to make the world manageable and calculable, the madness of philosophers imagined that in these categories it presented the concept of another `true' world, to which the one in which man lives does not correspond.
	[\ldots]
This is the greatest error that has ever been committed.
One believed one possessed a criterion of reality in the form of reason, while in fact, one possessed them in order to become master of reality.
In order to misunderstand reality in a shrewd manner.
And behold: now this world came to seem false, and precisely on account of the properties that constitute its reality: change, becoming, multiplicity, opposition, contradiction, war.
And then, the entire fatal error was there.''
	\vspace{.25cm}\hrule
	\hfill Friedrich Nietzsche, \citetitle{nietzsche88}, 1888
\end{quote}

\section{How Not to Do Optometry}

Imagine if there was a school of thought in the ocular sciences that insisted that humans are endowed with an ``irrational'' visual system.
They could find evidence of their view everywhere.

Most of their fruitful empirical work would consist in putting humans in unfamiliar or unnatural viewing situations, perhaps with optical illusions or color distinguished by an imperceptible degree.
They would run their experiments expectantly, and to no one's surprise, would find that humans are terribly bad at performing these rarified tasks.

They would talk about the theory behind it all: objects are actually just agglomorations of atoms, which the human visual system fails to see, qualia like color are utter illusions, in reality they are only light of different wavelengths that our eyes erroneously perceive as these invented colors, which exist nowhere outside of the mind.
In short, the human visual system is mostly a lie, and can easily be spoofed and tricked.

Visual science popularizers would write books about how you can overcome your irrational eyes.
People would see new technology, like computer screens, and talk about how poorly suited the human eyes are at adjusting to prolonged use of these evolutionarily unprecedented circumstances.

No doubt there might be a good bit of popular snobbery, where those educated in this kind of science would look down on rubes who insisted on relying on their own irrational eyes rather than the formal deductions of Rational Eye Theory.
When a person was told that they perceived something ``irrational'', they would view it as a problem to be fixed.

This fantasy may sound somewhat absurd, but it's not to dissimilar too the world we live in.
Visual scientists don't subscribe to an ideology so queer, but many economists, cognitive scientists, decision theorists and others hold, more or less the same idea about human cognition, implicitly or explicitly.

For the last 60 or 70 years, there has been a fixation in these fields with human ``irrationality'' generally.
Part of this is a necessary theoretical defense mechanism: after several centuries of economic modelling, it was realized that humans don't necessarily follow the assumptions of whatever economic theory.
The error of economists had to be an error of the assumptions, and the most frequently blamed was that amorphous assumption of ``rationality''.

We are told not just that the human mind is equipped with some heuristics, but that these heuristics, seeing that they fail to match a Platonic standard of ``logic'' are in fact ``irrational''.
If heuristics lead us to correct conclusions in complex domains, even systematically so, that is a kind of accident, like a broken clock being right twice a day.
It's quite common, like the optometrist who ascribes to Rational Eye Theory, to create rarified experimental environments where decision heuristics are taken \textit{ad absurdum}, put participants in them, then react in surprise that these novel situations elicit ``irrational'' responses.

\subsection{What's Wrong with Rational Eye Theory?}

Rational Eye Theory doesn't do faulty research.
They don't necessarily publish false things, in fact the research is interesting and important for understanding the visual system.

Rational Eye Theory errs on two points.
First, it attributes a normative value to the visual system.
Even when proponents insist that Rational Eye Theory is merely positive, they still use very normative words to lambast what fails to meet its standard (``irrational'', ``biased'' etc.)
It creates an artificial Platonic ideal of what ``rational'' vision is, an ideal totally disconnected from the actual use of the eye.
Calling this ideal ``rational'' or ``good'', it assumes that any tendency of the eye to not follow this invented standard is therefore ``bad'' or ``irrational''.

Second, it misses the whole reason vision exists.
Vision does not exist to solve sterile, contextless visual puzzles or survive optical illusions in experimental circumstances.
It is a tool for making the relevant distinctions in real life, and doing so quickly and efficiently using as few priors and as little statistical noise as possible.

This is the metric to which vision should be assessed.
While we might want to totally keep normative judgments away from science, if we were to call anything ``good'' or ``rational'', it should be this.
Indeed it is easy to argue, as I will in section \ref{models} that the difference between the employment of so-called heuristics and biases and the use of what we traditionally think of as deductive reason is a fundamental difference of quantity, not of adherence to truth of effectiveness.

And naturally, the same is true of those who like to imagine that humans are flawed for their ``irrationalities'' in decision-making.
We should not constantly evaluate normatively human decision-making by its adherence to an artificial standard of coherence in highly limited circumstances.
Humans may be Dutch Book-able in experimental circumstances, or in situations outside of their familiarity, but as I'll argue in section \ref{tails}, in the most important domain of practical behavior, in uncertainty, so-called ``irrationality'' in the form of precausion and avoidance of the unfamiliar is precisely what keeps them from disaster.

Instead, if we do judge human reliance on heuristics and biases, we should only do so in the environment where it developed and where it is actually suited to operate in: i.e. \emph{real life}.

Like vision, human reason and heuristics and biases are not supposed to reflect a Platonic ideal of the world, nor are the generalizations we can make about the different cognitive modules operating in life comparable or equivalent (meaning, they can seem incoherent and contradictory and still highly effective).
Human reason and heuristics exist to produce results, and like vision, body-motion, tactile sensation and all other human senses, each of these function in the domain they have developed in.

The sociological edge of this is that much of this has led to a public condemnation of intuitive or heuristic thinking.
In most times and places, reliance on heuristics (inborn or otherwise) is vastly superior to a deliberative (``rational'') consideration of problems.


\section{Organization}

I've organized this argument into numbered sections, but each is a holon in itself.
I am not presenting so much of a single thesis about reason, but a gestalt, and am doing so precisely because I reckon this misunderstanding of the human mind has many tendrils in need of addressing in distinct ways.

\section{The algorithm of deconstruction}

This tendency in the cognitive and decision sciences exists because it falls out of an easy and all-to-common algorithm of philosophical deconstruction which had become quite popular in the 20th century.
It's not new; one can refer to \textcite{nietzsche88}'s interdict against philosophers reproduced at the introduction to this document, which decries the same intellectual tendency.
One can also see the condemnations of this ``Platonism'', so-called, as a moral and political philosophy in \textcite{popper45} which amount to the same criticism.

This algorithm is simple.
(1) Invent an abstract ``rational'' metric and a narrow logical standard, ignoring what aspects of life deemed unimportant or insignificant \textit{a priori}.
As Nietzsche notes, this standard can indeed come from some genuinely practical use of reason.
(2) Make deductions in this logical system.
(3) Bring these deductions back into the real world.
(4) If the deductions fail to correspond to real-life behavior, conclude that real-life behavior is ``irrational''.
(5) If the deductions not only fail to correspond to real-life behavior, but they also produce bad alternatives, note that it's only a model useful only in abstract situations.
It's no different than an proverbial economist who, when chided for his blatant failures to predict or model events, retorts saying ``it's not my fault people aren't rational''.

This algorithm, while obviously never explicit, is still ubiquitous in the decision sciences, and is the typical methodology in the line of thought typically terms the ``Heuristics-and-Biases'' program.
\textcite{gigerenzer96}, in a reply to \textcite{kahneman96}, voices the same critique of this program, saying\ldots

\begin{quotation}
	``Most practical statisticians start by investigating the content of a problem, work out a set of assumptions, and, finally, build a statistical model based on these assumptions.
	The heuristics-and-biases program starts at the opposite end.
	A convenient statistical principle, such as the conjunction rule or Bayes's rule, is chosen as normative, and some real-world content is filled in afterward, on the assumption that only structure matters.''
\end{quotation}

That is, before thorough investigation of cognition, we are typical to first decide how cognitition \emph{should be} ordered.
To bring to bear a distinction common in linguistics, there's a subtle parellel between this and the contrast between linguistic description and linguistic prescription.
The heuristics-and-biases approach is adopting a tacit form of cognitive prescription, not only when they form overtly normative models of decision-making, but also when they craft narrow ``descriptions'' with somewhat loaded terminology, ``logical'', ``rational'' even ``optimization''.

Although endorsing the heuristics-and-biases program, \textcite[194]{massimo96} makes the same point, and I think, correctly implies that the difference between positive and normative theory in these situations is merely ``terminological'':

\begin{quotation}
	``Whether this is so because of normative conceptions of rationality, or simply with subject being `right' or `wrong,' is up to a point [of] terminological dispute. What counts is that everyone's life would be better if we learned to correct many of these spontaneous probabilistic intuitions of ours, and there are `reasons' that we would be better off.''
\end{quotation}

Thus, while some economists or decision theorists might hide behind purported claims to notional positivity, there is something fundamentally recommendational about the heuristics-and-biases program.
Indeed, it focuses so strongly on behavior which is apparently paradoxical and perplexing without too much effort into the \emph{origins}, synchronically or ``diachronically'' (i.e. evolutionarily) in the origin of these perplexities.

\section{Not so irrational after all}

This oversight means that the heuristics-and-biases program is apt to find a plethora of false positives in human ``irrationality''.
That is, if our perogative is to find irrationality, and we don't expect irrational behavior to be motivated, it's very sensible that we will misunderstand behaviors for which we've overlooked the context.

Indeed, we should expect that the field will produce many of these flase positives over time. \textcite[14--15]{gigerenzer08} does well to list some of these reversals,  and it is worth repeating them in part.



Some biases are not so much biases at all, but results of improper context in experimental circumstances.
The results of \textcite{sedlmeier98} are likely an example of this: the so-called Availability Bias, while present in an original results of \textcite{tversky73}, disappear when the letters are sampled rather than merely selected.

Something similar could be said, as noted by \textcite{lopes92}, that some superficial ``overconfidence'' biases are the result of sensible assessments of tail risks.
That is, a large majority of drivers say they drive ``better than the average'', and while this may seem like a contradiction, it's true when we realize that traffic accidents are rare and \emph{highly skewed}: a small minority drastically produce far more than their share of accidently, meaning the majority \emph{is} better than they would be in a statistically equal distribution.

As \textcite{hertig05} note, the ``irrational'' overestimation of rare chances and the underestimation of more likely ones is not so much an error as a habit of \emph{hedging against error}: one assumes that their information is incomplete, and as such all probabilities should regress to the mean.

The same is true of the so-called \textit{Hard-Easy Effect}, where humans tend to overestimate performance on difficult tasks while underestimating performance on the mundane: the effect is as well a self-critical expectation of regression to the mean.
Indeed in \textcite{juslin00}'s revision of this ``bias'', they compare the those who insist on general human irrationality to the belief that, because the horizon is flat, therefore the world must be also: a ``naive empiricism''.


\section{Where do biases end and optimization begin?}

Indeed on closer evaluation, not only should it be clear that heuristics and biases are fundamentally consistent, but that the boundary between them and deliberative logic is an issue of scale. Indeed, in a manner of thinking, \emph{all} human cognitive abilities are the results of heuristics and biases, including optimization.

First, take a popular psychological mind hack.
Say we ask experiment participants (or random people on the street) to give their estimates for math problems.
A common so-called ``glitch'' in human computation is shown by giving people the following who problems:

\begin{exe}
	\ex $1\times2\times3\times4\times5\times6\times7\times8$
	\ex $8\times7\times6\times5\times4\times3\times2\times1$
\end{exe}

Examinining these two together, it should be clear that they are the same problem, and thus any rational estimate of etheir should be the same.
The answer to both of these math problems is 40,320.

However, people give tremendously different estimates to each if shown alone.
The first often receives an estimate in the hundreds, the second in the thousands.

But a good question then is \textit{What is the \emph{unbiased}, rational estimate?}
To reevoke the comparison of heuristics to the ocular system, a decent question worth asking is ``what would a totally `rational', `optimized' and clear ocular system look like?''
Would it see atoms themselves?
Would it see qualia such as color and solidity that we know are illusions?

There are not so much good answers to these questions because there are \emph{not even coherent} answers to them.
In the same way that there is no ``unbiased'' way to use the visual system, there is no ``unbiased'' way to estimate these multiplicative products.

One of the pernitiously flawed metaphors when talking about biases is the idea that there is some otherwise sterile deliberative process generating well-reasoned estimates generally, which is only then obfuscated by a ``bias'' that puts its finger on the scale, changing the output.

What is actually happening is that cognitive heuristics are applying algorthmically to address problems, and when we notice the tendencies of the heuristics to missguess, we call that difference a ``bias''.
Hence, one might colloquially call our response to such math estimates as ``biased'', \emph{but there is no other way to estimate these products}.
There is an algorithm we learn in school to achieve the correct answer, but this has no cognitive reality and is a learned way of manipulating numbers that happens to yield the right 40,320.

Instead, what probably happens in each case is that we investigate the math problem and begin estimating the product moving left to right.
As our memory is taxed, our estimation becomes more fuzzy until we simply give up and give a very general guess, paying little attention to the later numbers.

One can reject this description of this algorithm for its debilitation by cognitive constraints, therefore making it ``irrational'', but that the important point is that heuristics and biases are \emph{the only} real way cognition proceeds in this case, and when we start to act ``rationally'' we are really just compounding different heuristics together.

\section{They are all models\label{models}}

We should take a step back to be clear for a second.
Nowhere am I suggesting that what is called reason is \emph{bad} \textit{per se}, rather I object to a na{\"i}ve supremacy attributed to it.
In this vision, optimization is the proper measure of the world, not other mental tools, and if optimization fails to shine in a particular domain, that is no fault of its own.

But on closer analysis it should be clear that the borders between optimization and the use of heuristics is are fuzzy at best.
In reality, all implementations of both heuristics and rational deliberation are models of the world spread about a gradient of complexity.

A heuristic is a simple model, often with only one variable input, which is sometimes binary at that.
The logical form of a heuristic as a model can be as simple as $A{\rightarrow}B$.
Such a model is as economical as computationally possible and can lead quite easily to a solution.

Reason too is a model, albeit a more complex one.
In the most semantically bleached form, it involves various propositions (sometimes statistical in nature) and logical connectives which join the inputs in suchy a way to give the desired answer.

This more complex model necessitates generalizing, checking for contradiction among subtheses (effectively \textit{proving} or \textit{disproving}).
That means it is a more mentally taxing process, and more likely to fall prey to short-term memory constraints or general error.

Thus, when we think about the difference between reliance on heurisitc and reliance on reason, we are really thinking of a difference of quantity.

There is not necessarily less logic in a heuristic than there is in more advanced logical optimization.
One might object saying that reason imputes causality or some deeper level of relation than a heuristic, but it should be clear that this is an illusion.

The proper way, I think, to look reason/optimization or what is called System 2 in \textcite{kahneman11} and similar works, is not so much as a discrete sense of deliberation \emph{per se}, but as a general tool that recruits other narrow mental faculties, heuristics and microtools to the set purpose.

The conscious process of reasoning or deliberation or optimization is often one in which the reasoner trolls his mind for more variables and heuristics to load into the model.
The simple heuristic solution may already be there, and he is cognizant of it, but he wants to ``be sure'' by adding all the variables he can think of to his implicit model.
This mental process amounts to loading on more and more heuristics to test the theoretical efficacy of the first.

But the addition of more model variables comes with diminishing returns either sooner or later.
Formal models often show poorer performance than heuristics precisely because they have much more statistical noise.
Evoking irrelevant mental faculties to determine an answer come with a cost that may not be compensated by higher accuracy, in fact, past a certain level, it must overstay its use.

At the worst the optimization process fails to converge on a solution in what has been called ``paralysis by analysis'' or ``the paradox of choice'' \parencite{schwartz04}.
This amounts to an integration of so many variables into a model of decision-making that the decision-making itself becomes impossible due to the overlapping uncertainties.

More often, I think, overthinking simply causes bad results.
When a model, or a logical process is too complex, its variables stop bringing marginal utility, making the ``fast and frugal [single] heuristic'' not just cheap, but far more accurate.


\section{On Fat Tails and Uncertainty\label{tails}}

That said, there is one area, totally absent in experimental circumstances, but ubiquitous in real-life that logical deduction \emph{cannot} by definition model.
That is raw uncertainty, especially in circumstances when potential down-sides are cataclysmic: Black Swans.

A common denominator behind many ``biases'' is an ``irrational'' fear of the unknown, the alien and a bias for the familiar.
This comes in many forms, from an experimental participant sticking at a suboptimal, yet familiar choice to him fundamentally not trusting what an experimenter has told him about an experiment.

In the face of uncertainty, especially in the face of potentially dangerous uncertainty, what is statistically ``rational'' changes dramatically.
As \textcite{taleb18} puts it ``Outcomes that entail ruin make sequences path dependent and noncommutative''.
This means that average payoff, even expected utility are dangerous things to consider when there is even a small risk of getting ``ruined''.
Experimental regulations prevent us from threatening participants with genuine ``ruin'', but even if we could, the relevant real-world situation is not only risky ruin, but \emph{uncertain} ruin.

This makes established paths highly rational.
If I can avoid substantial risk, or uncertainty, any sensible person would be willing to forego otherwise larger average payoffs or expected utility.
But since these situations are difficult to reproduce in experimental circumstances, there has arisen the idea that these hedging behaviors on the part of humans are useless.
A typical observer paradox.

Much of human behavior, especially that most seemingly ``irrational'' is the result of us keeping away from uncertainty.
\textcite{taleb12} even hypothesizes that the most ``irrational'' thing that humans do, religion, is largely a helpful antidote to engaging in uncertainty.
In an era of iatrogenic and uncertain medical treatment (today and in yesteryear) it's a much more ecologically rational thing to appease one's felt need to ``do something'' about a non-severe medical condition by praying at the Temple of Apollo rather than going to a doctor and taking fragilizing and potentially harmful medication.
Prayer is powerful precisely because it does nothing.


\textcite{taleb15} has even gone so far as to claim that ``moral failures'' such as procrastination are ecologically rational: as most non-acute problems, resolve themselves over time, or more convenient solutions will arise to these problems.
Just yesterday I was reminded of this as I had, for two months, procrastinated buying a shuttle ticket I needed for a flight out of Phoenix.
Sure enough, I received a random call from a friend who volunteered to drive me herself, which means my procrastination saved me an easy \$50.
Even more, if I hadn't procrastinated writing up this paper until the last minute, I wouldn't be able to include this anecdote in the paper (along with many other things I ran into).

\section{Heuristics as Exaptation}

%\textcite{schulz11}

It has been argued \parencite{massimo89,gould91} that

Part of this stems from a general skepticism of the coherence of natural ``selection'' as a concept (see \textcite{fodor10}).
With respect to the construction of evolutionary \textit{just-so} stories, this is an important critique, but our narrative here does not necessarily relied on it.

That is---to say that heuristics, and how and when they are used are all well calibrated to the human environment is not to say that the heuristics are physiologically present in the mind-brain, nor is it to say that they were chosen by natural selection.
It's equally as easy to these heuristics were \emph{exaptive} \parencite{buss98} or spandrels in the sense of \textcite{gould79}, brought about by happenstance and repurposed for new ends.

Indeed, \textcite{gigerenzer08} addresses heuristics not as evolutionarily selected, but saying that they ``exploit evolved capacities'', or in other words, as exaptations.
Heuristics in \textcite{gigerenzer08}'s terminology, mental heuristics are not ingrained mental tunnels, but an \emph{adaptive toolbox} or different general capacities which are evoked \emph{ad hoc} to be useful when they are found to be so.

The same sentiment is echoed by \textcite{anderson90}, where \emph{cognition itself} is adaptive.
Heuristics and biases are often



\section{The neurology of non-rational reason}

Research such as that in \textcite{hsu05} is fundamental as it paints a more complete picture of how ``unrational'' parts of the brain come into play.
The amygdala and other ``emotional'' centers of the brain are recruited into decision-making precisely when those typical centers characteristic of logical optimization fall short of making a holistic and contexted decision.

And, when brain damage makes the amygdala uninvokable, decision making \emph{gets worse}.
A person

\section{Abstract Reasoning}

One example of this is the secular rise in IQ scores generally noted by psychologists, eventually dubbed ``the Flynn Effect'' by \textcite{herrnstein94} (after \textcite{flynn87}).

Efforts to attribute this rise to a spectacular genetic factor are not generally accepted or are variously vexed \parencite{woodley11}, while even recent research seems to vidicate that the Flynn Effect is most saliently correlated with an increased reliance on abstract reasoning \parencite{must16}.

\printbibliography

\end{document}

%Ignoring uncertainty, or worse, treating it as risk makes us Dutch Bookable to nature.
